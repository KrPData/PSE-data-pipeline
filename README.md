# PSEâ€‘dataâ€‘pipeline

**ETL pipeline dla danych z PSE** â€” projekt automatyzujÄ…cy pobieranie, transformacjÄ™ i Å‚adowanie danych m.in. krajowej generacji KSE, parametrÃ³w mocy bilansujÄ…cych itp. do bazy danych PostgreSQL. Daje moÅ¼liwoÅ›Ä‡ kofiguracji pod preferowane dane z PSE API.

## ğŸ§° Co to jest  

- Projekt pobiera dane z publicznego API Polskich Sieci Elektroeneretycznych dotyczÄ…cych rynku energii
- Transformuje dane â€” waliduje, porzÄ…dkuje strukturÄ™, normalizuje formaty  
- Åaduje przetworzone dane do bazy PostgreSQL i zapasowych plikÃ³w CSV
- MoÅ¼e byÄ‡ odpalany jako harmonogram â€” idealny jako ETL do dalszej analizy lub dashboardÃ³w  

## ğŸš€ Technologie  

- Python
- Pandas  
- SQLAlchemy + psycopg2 (PostgreSQL)  
- Struktura projektu: foldery `extract/`, `transform/`, `load/`, `config/`, `db/`
- Plik `requirements.txt` z wymaganymi bibliotekami  
- Plik `.gitignore` aby nie dublowaÄ‡ danych, cache, itp.  
- Licencja: MIT  

## ğŸ“¦ Jak uruchomiÄ‡ we wÅ‚asnym Å›rodowisku

1. Sklonuj repozytorium  
   ```bash
   git clone https://github.com/KrPData/PSE-data-pipeline.git
   cd PSE-data-pipeline
2. pip install -r requirements.txt

    ```
    DB_HOST=localhost  
    DB_PORT=5432  
    DB_NAME=<nazwa_bazy>  
    DB_USER=<uÅ¼ytkownik>  
    DB_PASS=<hasÅ‚o>  
    ```

3. python run_pipeline.py

## ğŸ—‚ï¸ Struktura katalogÃ³w
```
PSE-data-pipeline/
â”œâ”€â”€ extract/ # pobieranie danych
â”œâ”€â”€ transform/ # przetwarzanie danych
â”œâ”€â”€ load/ # Å‚adowanie do bazy i plikÃ³w CSV
â”œâ”€â”€ db/ # pliki db / schematy / migracje
â”œâ”€â”€ config/ # konfiguracje, pliki .env
â”œâ”€â”€ utils/ # funkcje pomocnicze
â”œâ”€â”€ run_pipeline.py # gÅ‚Ã³wny skrypt wykonawczy
â”œâ”€â”€ requirements.txt
â””â”€â”€ .gitignore
```

Licencja MIT

